{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning Tuning:\n",
    "  * Use Resnet50 to reduce model size and improve accuracy \n",
    "      * see [base model info comparison](https://medium.com/@sidereal/cnns-architectures-lenet-alexnet-vgg-googlenet-resnet-and-more-666091488df5)\n",
    "      * [resnet50 blog](https://towardsdatascience.com/understanding-and-coding-a-resnet-in-keras-446d7ff84d33)\n",
    "     \n",
    "  * base_model: use RESNET50 and freeze at bottleneck layer (stop right before 2D AVERAGE POOL) \n",
    "  * top_model: tune dense layers (parameters are inspired by a few sources)\n",
    "     * [source1](https://www.kaggle.com/suniliitb96/tutorial-keras-transfer-learning-with-resnet50)\n",
    "     * [source2](https://www.kaggle.com/pmigdal/transfer-learning-with-resnet-50-in-keras)\n",
    "     * [source3](https://towardsdatascience.com/transfer-learning-for-image-classification-using-keras-c47ccf09c8c8)\n",
    "\n",
    "\n",
    "\n",
    "  \n",
    "---\n",
    "#### This cell is required in order to use GPU for running the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0904 23:45:39.479041 140378366863168 deprecation_wrapper.py:119] From /home/mindy/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W0904 23:45:39.479885 140378366863168 deprecation_wrapper.py:119] From /home/mindy/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True  # dynamically grow the memory used on the GPU\n",
    "config.log_device_placement = True  # to log device placement (on which device the operation ran)\n",
    "                                    # (nothing gets printed in Jupyter, only if you run it standalone)\n",
    "sess = tf.Session(config=config)\n",
    "set_session(sess)  # set this TensorFlow session as the default session for Keras\n",
    "keras.backend.get_session().run(tf.global_variables_initializer())  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import class_weight\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras import optimizers\n",
    "from keras.applications.resnet50 import ResNet50, preprocess_input\n",
    "from keras.layers import Dropout, Flatten, Dense, GlobalAveragePooling2D\n",
    "from keras.models import Sequential, Model, model_from_json\n",
    "from keras_preprocessing.image import ImageDataGenerator\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  import total dataframe\n",
    "\n",
    "df = pd.read_pickle('../pickle_files/pic_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset a dataframe \n",
    "def create_test_train(df,label):\n",
    "    '''\n",
    "    df: dateframe of pic_ids \n",
    "    label: eyewear,hat, or beard, a string\n",
    "    return: test and train df\n",
    "    '''\n",
    "    sub_set = df[['pic_id',label]]  # subset the label dataframe \n",
    "    X_train, X_test, y_train, y_test = train_test_split(sub_set['pic_id'], sub_set[label],\n",
    "                                                        stratify = sub_set[label],\n",
    "                                                        test_size = 0.2)\n",
    "    df_train = pd.concat([X_train,y_train], axis=1)\n",
    "    df_test = pd.concat([X_test,y_test], axis=1)\n",
    "    return df_train, df_test\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_generator(train_df, test_df,label,shuffle,batch_size,\n",
    "                    rescale, preprocess_func, target_size,\n",
    "                    class_mode, only_testing = False):\n",
    "    '''\n",
    "    train_df, test_df: dataframe for train and test \n",
    "    label: eyewear,hat, or beard, a string\n",
    "    shuffle: weather to shuffle or not based upon feature extractor or not \n",
    "    batch_size: how many pictures per batch\n",
    "    preprocess_func: vgg16 preprocess or ResNet50\n",
    "    rescale: 1./255 or None\n",
    "    target_size: (224,224) or (150,150)\n",
    "    class_mode: none or binary \n",
    "    return:\n",
    "    train generator and test generator \n",
    "    '''\n",
    "    generators =[]\n",
    "    if not only_testing:\n",
    "        traingen = ImageDataGenerator(\n",
    "            rescale = rescale,\n",
    "            zoom_range= [0.8,1.7],\n",
    "            shear_range=0.2,\n",
    "            brightness_range=[0.5,1.5],\n",
    "            rotation_range = 40,\n",
    "            width_shift_range=0.2,\n",
    "            height_shift_range=0.2,\n",
    "            horizontal_flip=True,\n",
    "            preprocessing_function=preprocess_func)\n",
    "        \n",
    "        train_generator = traingen.flow_from_dataframe(\n",
    "            dataframe=train_df,\n",
    "            directory='../data/pics',\n",
    "            x_col='pic_id',\n",
    "            y_col=label,\n",
    "            batch_size=batch_size,\n",
    "            shuffle = shuffle,\n",
    "            target_size=target_size,\n",
    "            class_mode = class_mode)\n",
    "        generators.append(train_generator)\n",
    "        if class_mode:\n",
    "        # create classweights for train \n",
    "            classweights = class_weight.compute_class_weight(\n",
    "                'balanced',np.unique(train_generator.classes),train_generator.classes)\n",
    "            generators.append(classweights)\n",
    "    \n",
    "    testgen = ImageDataGenerator(\n",
    "        rescale = rescale,\n",
    "        preprocessing_function=preprocess_func)\n",
    "    \n",
    "    test_generator = testgen.flow_from_dataframe(\n",
    "        dataframe=test_df,\n",
    "        directory='../data/pics',\n",
    "        x_col='pic_id',\n",
    "        y_col=label,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        target_size=target_size,\n",
    "        class_mode=class_mode)\n",
    "    generators.append(test_generator)\n",
    "    \n",
    "    return generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = create_test_train(df,'eyewear')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get bottleneck features to tune top models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_bottleneck_features(df, label, batch_size, model_type, preprocess_func, rescale,\n",
    "                            shuffle,target_size,class_mode):\n",
    "    '''\n",
    "    inputs:\n",
    "    df: dataframe for all eyewear, hat, and beard \n",
    "    label: a string, eyewear, hat, or beard\n",
    "    batch_size: process images in batches\n",
    "    model_type: ResNet50 or VGG16\n",
    "    shuffle: weather to shuffle or not based upon feature extractor or not \n",
    "    preprocess_func: vgg16 preprocess or ResNet50\n",
    "    rescale: 1./255 or None\n",
    "    target_size: (224,224) or (150,150)\n",
    "    class_mode: none or binary \n",
    "    outputs:\n",
    "    saves bottleneck features inside folder tuning_data as npy file\n",
    "    '''\n",
    "    # intialize the model, vgg16 or ResNet50 \n",
    "    # make sure not to train the top layers \n",
    "    base_model = model_type(weights = 'imagenet',include_top = False)\n",
    "    # generate test_train df\n",
    "    train_df, test_df = create_test_train(df,label)\n",
    "    # create train_generator and test_generator to get bottleneck inputs for train and test df \n",
    "    # make sure shuffle is False so we know the label follows the sequence of the dataframe \n",
    "    # so we can tune top_model \n",
    "    generators = create_generator(train_df=train_df,\n",
    "                                                   test_df=test_df,\n",
    "                                                   label=label,\n",
    "                                                   shuffle=shuffle,\n",
    "                                                   rescale=rescale,\n",
    "                                                   preprocess_func=preprocess_input,\n",
    "                                                   batch_size=16,\n",
    "                                                   target_size=target_size,\n",
    "                                                   class_mode=class_mode)  \n",
    "    train_generator, test_generator = generators[0],generators[1]\n",
    "    \n",
    "    # get features saved as .npy in tunign_data folder \n",
    "    bottleneck_features_train = base_model.predict_generator(\n",
    "        train_generator, train_df.shape[0]//batch_size)\n",
    "    np.save(open(f'../tuning_data/resnet_data/untracked_resnet50/bottleneck_features_train_{label}_resnet50_beta.npy','wb'),\n",
    "           bottleneck_features_train)\n",
    "    \n",
    "    bottleneck_features_test = base_model.predict_generator(\n",
    "        test_generator, test_df.shape[0]//batch_size)\n",
    "    np.save(open(f'../tuning_data/resnet_data/untracked_resnet50/bottleneck_features_test_{label}_resnet50_beta.npy','wb'),\n",
    "           bottleneck_features_test)\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save bottleneck_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0904 23:46:11.958368 140378366863168 deprecation_wrapper.py:119] From /home/mindy/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0904 23:46:11.959231 140378366863168 deprecation_wrapper.py:119] From /home/mindy/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0904 23:46:11.962897 140378366863168 deprecation_wrapper.py:119] From /home/mindy/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4479: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
      "\n",
      "W0904 23:46:12.282516 140378366863168 deprecation_wrapper.py:119] From /home/mindy/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "W0904 23:46:12.316155 140378366863168 deprecation_wrapper.py:119] From /home/mindy/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 522 validated image filenames.\n",
      "Found 131 validated image filenames.\n"
     ]
    }
   ],
   "source": [
    "train_df, test_df = save_bottleneck_features(df,'eyewear',16,ResNet50,preprocess_input,\n",
    "                         None,False,(224,224),None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet50_model(input_shape, dropout):\n",
    "    '''\n",
    "    input_shape: input_shape for pooling layer \n",
    "    dropout:percentage for Dropout layer \n",
    "    optimizer: optimizer for model compiles \n",
    "    returns resnet50 model\n",
    "    '''\n",
    "    model = Sequential()\n",
    "    model.add(GlobalAveragePooling2D(input_shape=input_shape))\n",
    "    model.add(Dense(512,activation='relu'))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(1,activation='sigmoid'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vgg16_model(input_shape,dropout):\n",
    "    '''\n",
    "    input_shape: input_shape for pooling layer \n",
    "    dropout:percentage for Dropout layer \n",
    "    optimizer: optimizer for model compiles \n",
    "    returns vgg16 model\n",
    "    '''\n",
    "     # build top model\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=input_shape))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    return model    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick tuning of top models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_top_model(train_df, test_df, epoch, batch_size, label, model_type,rescale,preprocess_func,\n",
    "                   target_size):\n",
    "    '''\n",
    "    inputs:\n",
    "    train_df, test_df: dataframes returned from save_bottleneck_features functions \n",
    "    epoch: num of epochs in fit \n",
    "    batch_size: same as image generator batch size \n",
    "    label: a string, eyewear, hat, or beard\n",
    "    model_type: resnet50 or vgg16\n",
    "    output:\n",
    "    saves model weights in a folder \n",
    "    '''\n",
    "    train_data = np.load(open(f'../tuning_data/resnet_data/untracked_resnet50/bottleneck_features_train_{label}_resnet50_beta.npy','rb'))\n",
    "    # make sure train_data and train_label have same num of samples\n",
    "    train_label = np.array(train_df[label].map({'0':0, '1':1}))[:-(train_df.shape[0]%batch_size)]\n",
    "    \n",
    "    test_data = np.load(open(f'../tuning_data/resnet_data/untracked_resnet50/bottleneck_features_test_{label}_resnet50_beta.npy','rb'))\n",
    "    test_label = np.array(test_df[label].map({'0':0, '1':1}))[:-(test_df.shape[0]%batch_size)]\n",
    "    \n",
    "    # build top model\n",
    "    if model_type == 'resnet50':\n",
    "        model = resnet50_model(train_data.shape[1:],0.25)\n",
    "    if model_type == 'vgg16':\n",
    "        model = vgg16_model(train_data.shape[1:],0.5)\n",
    "        \n",
    "    model.compile(optimizer=optimizers.Adam(),\n",
    "                 loss='binary_crossentropy',\n",
    "                 metrics=['accuracy'])\n",
    "\n",
    "    # checkpoint for best weights \n",
    "    filepath=f\"../tuning_data/resnet_data/untracked_resnet50/best_bottleneck_resnet50_model_{label}_beta.h5\"\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "    callbacks_list = [checkpoint]\n",
    "    \n",
    "    classweight = create_generator(train_df, test_df,label,False,batch_size,\n",
    "                    rescale, preprocess_func, target_size,\n",
    "                    'binary', only_testing = False)[1]\n",
    "    \n",
    "    model.fit(train_data, train_label,\n",
    "             epochs=epoch,\n",
    "             batch_size=batch_size,\n",
    "             validation_data=(test_data,test_label),\n",
    "             callbacks=callbacks_list,\n",
    "             class_weight = classweight)\n",
    "    del model\n",
    "    keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### run train_top_model and save results in tuning_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0904 23:46:37.476774 140378366863168 deprecation.py:506] From /home/mindy/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0904 23:46:37.494773 140378366863168 deprecation_wrapper.py:119] From /home/mindy/anaconda3/lib/python3.7/site-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0904 23:46:37.498836 140378366863168 deprecation.py:323] From /home/mindy/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 522 validated image filenames belonging to 2 classes.\n",
      "Found 131 validated image filenames belonging to 2 classes.\n",
      "Train on 512 samples, validate on 128 samples\n",
      "Epoch 1/50\n",
      "512/512 [==============================] - 1s 2ms/step - loss: 0.5711 - acc: 0.7578 - val_loss: 0.4989 - val_acc: 0.7891\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.78906, saving model to ../tuning_data/resnet_data/untracked_resnet50/best_bottleneck_resnet50_model_eyewear_beta.h5\n",
      "Epoch 2/50\n",
      "512/512 [==============================] - 0s 295us/step - loss: 0.2263 - acc: 0.9023 - val_loss: 0.1716 - val_acc: 0.9062\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.78906 to 0.90625, saving model to ../tuning_data/resnet_data/untracked_resnet50/best_bottleneck_resnet50_model_eyewear_beta.h5\n",
      "Epoch 3/50\n",
      "512/512 [==============================] - 0s 295us/step - loss: 0.1132 - acc: 0.9648 - val_loss: 0.1632 - val_acc: 0.9531\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.90625 to 0.95312, saving model to ../tuning_data/resnet_data/untracked_resnet50/best_bottleneck_resnet50_model_eyewear_beta.h5\n",
      "Epoch 4/50\n",
      "512/512 [==============================] - 0s 288us/step - loss: 0.0668 - acc: 0.9805 - val_loss: 0.1790 - val_acc: 0.9531\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.95312\n",
      "Epoch 5/50\n",
      "512/512 [==============================] - 0s 291us/step - loss: 0.0789 - acc: 0.9766 - val_loss: 0.1464 - val_acc: 0.9531\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.95312\n",
      "Epoch 6/50\n",
      "512/512 [==============================] - 0s 298us/step - loss: 0.0362 - acc: 0.9883 - val_loss: 0.1558 - val_acc: 0.9453\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.95312\n",
      "Epoch 7/50\n",
      "512/512 [==============================] - 0s 301us/step - loss: 0.0369 - acc: 0.9883 - val_loss: 0.2238 - val_acc: 0.9141\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.95312\n",
      "Epoch 8/50\n",
      "512/512 [==============================] - 0s 299us/step - loss: 0.0185 - acc: 0.9980 - val_loss: 0.1637 - val_acc: 0.9375\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.95312\n",
      "Epoch 9/50\n",
      "512/512 [==============================] - 0s 295us/step - loss: 0.0231 - acc: 0.9961 - val_loss: 0.1692 - val_acc: 0.9531\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.95312\n",
      "Epoch 10/50\n",
      "512/512 [==============================] - 0s 291us/step - loss: 0.0068 - acc: 1.0000 - val_loss: 0.1881 - val_acc: 0.9375\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.95312\n",
      "Epoch 11/50\n",
      "512/512 [==============================] - 0s 307us/step - loss: 0.0037 - acc: 1.0000 - val_loss: 0.1895 - val_acc: 0.9375\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.95312\n",
      "Epoch 12/50\n",
      "512/512 [==============================] - 0s 311us/step - loss: 0.0046 - acc: 1.0000 - val_loss: 0.1791 - val_acc: 0.9375\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.95312\n",
      "Epoch 13/50\n",
      "512/512 [==============================] - 0s 302us/step - loss: 0.0034 - acc: 1.0000 - val_loss: 0.1761 - val_acc: 0.9531\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.95312\n",
      "Epoch 14/50\n",
      "512/512 [==============================] - 0s 302us/step - loss: 0.0029 - acc: 1.0000 - val_loss: 0.1809 - val_acc: 0.9453\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.95312\n",
      "Epoch 15/50\n",
      "512/512 [==============================] - 0s 310us/step - loss: 0.0022 - acc: 1.0000 - val_loss: 0.1844 - val_acc: 0.9531\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.95312\n",
      "Epoch 16/50\n",
      "512/512 [==============================] - 0s 296us/step - loss: 0.0015 - acc: 1.0000 - val_loss: 0.1866 - val_acc: 0.9531\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.95312\n",
      "Epoch 17/50\n",
      "512/512 [==============================] - 0s 295us/step - loss: 0.0016 - acc: 1.0000 - val_loss: 0.1908 - val_acc: 0.9453\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.95312\n",
      "Epoch 18/50\n",
      "512/512 [==============================] - 0s 297us/step - loss: 0.0014 - acc: 1.0000 - val_loss: 0.1860 - val_acc: 0.9531\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.95312\n",
      "Epoch 19/50\n",
      "512/512 [==============================] - 0s 298us/step - loss: 0.0012 - acc: 1.0000 - val_loss: 0.1888 - val_acc: 0.9375\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.95312\n",
      "Epoch 20/50\n",
      "512/512 [==============================] - 0s 294us/step - loss: 0.0013 - acc: 1.0000 - val_loss: 0.1896 - val_acc: 0.9531\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.95312\n",
      "Epoch 21/50\n",
      "512/512 [==============================] - 0s 291us/step - loss: 9.8975e-04 - acc: 1.0000 - val_loss: 0.1897 - val_acc: 0.9453\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.95312\n",
      "Epoch 22/50\n",
      "512/512 [==============================] - 0s 292us/step - loss: 0.0011 - acc: 1.0000 - val_loss: 0.1931 - val_acc: 0.9297\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.95312\n",
      "Epoch 23/50\n",
      "512/512 [==============================] - 0s 295us/step - loss: 8.4941e-04 - acc: 1.0000 - val_loss: 0.1845 - val_acc: 0.9531\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.95312\n",
      "Epoch 24/50\n",
      "512/512 [==============================] - 0s 295us/step - loss: 7.7767e-04 - acc: 1.0000 - val_loss: 0.1923 - val_acc: 0.9453\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.95312\n",
      "Epoch 25/50\n",
      "512/512 [==============================] - 0s 299us/step - loss: 8.2187e-04 - acc: 1.0000 - val_loss: 0.1926 - val_acc: 0.9297\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.95312\n",
      "Epoch 26/50\n",
      "512/512 [==============================] - 0s 303us/step - loss: 6.5653e-04 - acc: 1.0000 - val_loss: 0.1963 - val_acc: 0.9297\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.95312\n",
      "Epoch 27/50\n",
      "512/512 [==============================] - 0s 287us/step - loss: 6.7408e-04 - acc: 1.0000 - val_loss: 0.1948 - val_acc: 0.9297\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.95312\n",
      "Epoch 28/50\n",
      "512/512 [==============================] - 0s 291us/step - loss: 6.0668e-04 - acc: 1.0000 - val_loss: 0.1933 - val_acc: 0.9375\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.95312\n",
      "Epoch 29/50\n",
      "512/512 [==============================] - 0s 305us/step - loss: 5.0815e-04 - acc: 1.0000 - val_loss: 0.1957 - val_acc: 0.9297\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.95312\n",
      "Epoch 30/50\n",
      "512/512 [==============================] - 0s 297us/step - loss: 4.9568e-04 - acc: 1.0000 - val_loss: 0.1938 - val_acc: 0.9453\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.95312\n",
      "Epoch 31/50\n",
      "512/512 [==============================] - 0s 302us/step - loss: 4.3224e-04 - acc: 1.0000 - val_loss: 0.1968 - val_acc: 0.9297\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.95312\n",
      "Epoch 32/50\n",
      "512/512 [==============================] - 0s 296us/step - loss: 5.8400e-04 - acc: 1.0000 - val_loss: 0.2013 - val_acc: 0.9375\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.95312\n",
      "Epoch 33/50\n",
      "512/512 [==============================] - 0s 295us/step - loss: 4.7570e-04 - acc: 1.0000 - val_loss: 0.1962 - val_acc: 0.9453\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.95312\n",
      "Epoch 34/50\n",
      "512/512 [==============================] - 0s 292us/step - loss: 4.8937e-04 - acc: 1.0000 - val_loss: 0.2033 - val_acc: 0.9297\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.95312\n",
      "Epoch 35/50\n",
      "512/512 [==============================] - 0s 292us/step - loss: 7.2752e-04 - acc: 1.0000 - val_loss: 0.1951 - val_acc: 0.9453\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.95312\n",
      "Epoch 36/50\n",
      "512/512 [==============================] - 0s 288us/step - loss: 4.3126e-04 - acc: 1.0000 - val_loss: 0.1969 - val_acc: 0.9375\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.95312\n",
      "Epoch 37/50\n",
      "512/512 [==============================] - 0s 279us/step - loss: 3.3173e-04 - acc: 1.0000 - val_loss: 0.1946 - val_acc: 0.9453\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.95312\n",
      "Epoch 38/50\n",
      "512/512 [==============================] - 0s 289us/step - loss: 4.2723e-04 - acc: 1.0000 - val_loss: 0.2008 - val_acc: 0.9453\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.95312\n",
      "Epoch 39/50\n",
      "512/512 [==============================] - 0s 296us/step - loss: 4.2654e-04 - acc: 1.0000 - val_loss: 0.2021 - val_acc: 0.9375\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 0.95312\n",
      "Epoch 40/50\n",
      "512/512 [==============================] - 0s 292us/step - loss: 3.2703e-04 - acc: 1.0000 - val_loss: 0.2042 - val_acc: 0.9453\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 0.95312\n",
      "Epoch 41/50\n",
      "512/512 [==============================] - 0s 295us/step - loss: 3.3145e-04 - acc: 1.0000 - val_loss: 0.2042 - val_acc: 0.9453\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.95312\n",
      "Epoch 42/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512/512 [==============================] - 0s 296us/step - loss: 3.1186e-04 - acc: 1.0000 - val_loss: 0.2063 - val_acc: 0.9375\n",
      "\n",
      "Epoch 00042: val_acc did not improve from 0.95312\n",
      "Epoch 43/50\n",
      "512/512 [==============================] - 0s 291us/step - loss: 3.2025e-04 - acc: 1.0000 - val_loss: 0.2054 - val_acc: 0.9453\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 0.95312\n",
      "Epoch 44/50\n",
      "512/512 [==============================] - 0s 293us/step - loss: 2.5207e-04 - acc: 1.0000 - val_loss: 0.2055 - val_acc: 0.9375\n",
      "\n",
      "Epoch 00044: val_acc did not improve from 0.95312\n",
      "Epoch 45/50\n",
      "512/512 [==============================] - 0s 303us/step - loss: 2.2296e-04 - acc: 1.0000 - val_loss: 0.2075 - val_acc: 0.9375\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 0.95312\n",
      "Epoch 46/50\n",
      "512/512 [==============================] - 0s 292us/step - loss: 2.8903e-04 - acc: 1.0000 - val_loss: 0.2093 - val_acc: 0.9453\n",
      "\n",
      "Epoch 00046: val_acc did not improve from 0.95312\n",
      "Epoch 47/50\n",
      "512/512 [==============================] - 0s 299us/step - loss: 2.2939e-04 - acc: 1.0000 - val_loss: 0.2115 - val_acc: 0.9375\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.95312\n",
      "Epoch 48/50\n",
      "512/512 [==============================] - 0s 293us/step - loss: 1.8925e-04 - acc: 1.0000 - val_loss: 0.2104 - val_acc: 0.9453\n",
      "\n",
      "Epoch 00048: val_acc did not improve from 0.95312\n",
      "Epoch 49/50\n",
      "512/512 [==============================] - 0s 292us/step - loss: 1.7630e-04 - acc: 1.0000 - val_loss: 0.2100 - val_acc: 0.9453\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 0.95312\n",
      "Epoch 50/50\n",
      "512/512 [==============================] - 0s 290us/step - loss: 1.8831e-04 - acc: 1.0000 - val_loss: 0.2121 - val_acc: 0.9375\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 0.95312\n"
     ]
    }
   ],
   "source": [
    "train_top_model(train_df, test_df, 50, 16, 'eyewear','resnet50',None,preprocess_input,target_size=(224,224))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine Tune Top Model to improve accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune_model(train_df, test_df,epoch, batch_size,label, \n",
    "                    model_type,dim, rescale, preprocess_func,print_model = False):\n",
    "    '''\n",
    "    inputs:\n",
    "    train_df, test_df: dataframes returned from save_bottleneck_features functions \n",
    "    epoch: num of epochs in fit \n",
    "    batch_size: same as image generator batch size \n",
    "    label: a string, eyewear, hat, or beard\n",
    "    model_type: resnet50 or vgg16\n",
    "    dim: 224 or 150 \n",
    "    rescale: None resnet50, 1./255 for vgg16\n",
    "    preprocess_func: depends on resnet or vgg16\n",
    "    return model \n",
    "    '''\n",
    "    # build model and freeze top layers\n",
    "    # input_shape: width, height, RGB (from image generator)\n",
    "    if model_type == 'resnet50':\n",
    "        base_model = ResNet50(weights='imagenet',include_top=False, input_shape=(dim,dim,3))\n",
    "        # build top model\n",
    "        top_model = resnet50_model(base_model.output_shape[1:],0.25)\n",
    "    if model_type == 'vgg16':\n",
    "        base_model = VGG16(weights='imagenet',include_top=False, input_shape=(dim,dim,3))\n",
    "        top_model = vgg16_model(base_model.output_shape[1:],0.5)\n",
    "    # load saved weights to fine tune parameters \n",
    "    top_model.load_weights(f'../tuning_data/resnet_data/untracked_resnet50/best_bottleneck_resnet50_model_{label}_beta.h5')\n",
    "    # add top model to model\n",
    "    model = Model(inputs=base_model.input, outputs=top_model(base_model.output))\n",
    "    # we will tune last 8 layers of the model: activation49 and fully connected layer \n",
    "    for layer in model.layers[:-5]:\n",
    "        layer.trainable = False\n",
    "    # we can tune the parameters for lr and momentum later to get better results\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "             optimizer=optimizers.SGD(lr=5e-5, momentum = 0.9),\n",
    "             metrics=['accuracy'])\n",
    "    # prepare train generator using data augmentation to battle small sample size \n",
    "    generators = create_generator(train_df,test_df, label,True,16,rescale,\n",
    "                                                    preprocess_func,(dim,dim),\n",
    "                                                    'binary')\n",
    "    train_generator, classweight, test_generator = generators[0], generators[1], generators[2]\n",
    "    \n",
    "    # checkpoint for best weights \n",
    "    filepath=f\"../tuning_data/resnet_data/untracked_resnet50/best_resnet50_model_5_layer_{label}_0.25dobeta_2.h5\"\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "    callbacks_list = [checkpoint]\n",
    "    \n",
    "    # run and fit model \n",
    "    result = model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_df.shape[0]//batch_size,\n",
    "    epochs=epoch,\n",
    "    validation_data=test_generator,\n",
    "    validation_steps=test_df.shape[0]//batch_size,\n",
    "    verbose=1,class_weight=list(classweight),\n",
    "    callbacks=callbacks_list\n",
    "    )\n",
    "    \n",
    "    if print_model:\n",
    "        model.summary()\n",
    "    \n",
    "    del model\n",
    "    keras.backend.clear_session()\n",
    "    return result                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save model history "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 522 validated image filenames belonging to 2 classes.\n",
      "Found 131 validated image filenames belonging to 2 classes.\n",
      "Epoch 1/50\n",
      "32/32 [==============================] - 8s 240ms/step - loss: 0.3431 - acc: 0.8484 - val_loss: 0.1550 - val_acc: 0.9375\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.93750, saving model to ../tuning_data/resnet_data/untracked_resnet50/best_resnet50_model_5_layer_eyewear_0.25dobeta_2.h5\n",
      "Epoch 2/50\n",
      "32/32 [==============================] - 8s 241ms/step - loss: 0.3027 - acc: 0.8847 - val_loss: 0.1928 - val_acc: 0.9217\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.93750\n",
      "Epoch 3/50\n",
      "32/32 [==============================] - 8s 248ms/step - loss: 0.2489 - acc: 0.9106 - val_loss: 0.1807 - val_acc: 0.9217\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.93750\n",
      "Epoch 4/50\n",
      "32/32 [==============================] - 7s 223ms/step - loss: 0.2516 - acc: 0.8981 - val_loss: 0.1292 - val_acc: 0.9391\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.93750 to 0.93913, saving model to ../tuning_data/resnet_data/untracked_resnet50/best_resnet50_model_5_layer_eyewear_0.25dobeta_2.h5\n",
      "Epoch 5/50\n",
      "32/32 [==============================] - 7s 220ms/step - loss: 0.2772 - acc: 0.8984 - val_loss: 0.1732 - val_acc: 0.9043\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.93913\n",
      "Epoch 6/50\n",
      " 7/32 [=====>........................] - ETA: 3s - loss: 0.1929 - acc: 0.9107"
     ]
    }
   ],
   "source": [
    "model_history = fine_tune_model(train_df,test_df,50,16,'eyewear','resnet50',\n",
    "                                224,None,preprocess_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "highest_val_acc, highest_train_acc = max(model_history.history['val_acc']), max(model_history.history['acc'])\n",
    "print(f'highest test accuracy: {highest_val_acc}')\n",
    "print('------------------')\n",
    "print(f'highest train accuracy: {highest_train_acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowest_val_loss, lowest_train_loss = min(model_history.history['val_loss']), min(model_history.history['loss'])\n",
    "print(f'lowest test loss: {lowest_val_loss}')\n",
    "print('------------------')\n",
    "print(f'lowest train loss: {lowest_train_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(model_history.history['val_acc'], color = 'red', label = 'test')\n",
    "plt.plot(model_history.history['acc'], color = 'blue', label = 'train')\n",
    "plt.title('accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(model_history.history['val_loss'], color = 'red', label = 'test')\n",
    "plt.plot(model_history.history['loss'], color = 'blue', label = 'train')\n",
    "plt.title('loss')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save model artchitecture \n",
    "# resnet50_model_8_up = model_history.model\n",
    "# resnet50_model_8_up_json = resnet50_model_8_up.to_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # serialize model to JSON\n",
    "# with open(\"../tuning_data/resnet_data/resnet50_model_8_up.json\", \"w\") as json_file:\n",
    "#     json_file.write(resnet50_model_8_up_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load json and create model\n",
    "def model_testing(df, model_path, weight_path, label, shuffle = False,\n",
    "                 batch_size = 32, rescale=None,preprocess_func=preprocess_input,\n",
    "                 target_size = (224,224),class_mode ='binary', only_testing = True):\n",
    "    '''\n",
    "    df: the entire picture df \n",
    "    model_path: json model path\n",
    "    weight_path: path for weights \n",
    "    label: eyewear, hat, or beard\n",
    "    '''\n",
    "    # check the whole dataframe\n",
    "    sub_set = df[['pic_id',label]] \n",
    "    json_file = open(model_path, 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    loaded_model = model_from_json(loaded_model_json)\n",
    "    # load model weight yewear_decay_\n",
    "    loaded_model.load_weights(weight_path)\n",
    "    # test test data\n",
    "    data_generator= create_generator(None, df,label,shuffle,batch_size,\n",
    "                    rescale, preprocess_func, target_size,\n",
    "                    class_mode, only_testing = only_testing)\n",
    "    data_generator=data_generator[0]\n",
    "    labels = df[label].map({'1':1, '0':0})\n",
    "    \n",
    "    loaded_model.compile(loss='binary_crossentropy',\n",
    "             optimizer=optimizers.SGD(),\n",
    "             metrics=['accuracy']\n",
    "                        )\n",
    "    y_pred = np.around(loaded_model.predict_generator(data_generator, workers=8))\n",
    "    y_pred = [1-x for x in y_pred] # for old model \n",
    "    fpr, tpr, thresholds = roc_curve(labels, y_pred)\n",
    "    area = roc_auc_score(labels, y_pred)\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % area)\n",
    "    plt.legend(loc = 'lower right')\n",
    "    plt.plot([0, 1], [0, 1],'r--')\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.show()\n",
    "    print(confusion_matrix(labels, y_pred))\n",
    "    print(classification_report(labels, y_pred))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eyewear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_testing(df=df,model_path='../tuning_data/resnet_data/resnet50_model_5_up.json',\n",
    "             weight_path='../tuning_data/resnet_data/untracked_resnet50/best_resnet50_model_5_layer_eyewear_0.25dobeta.h5',\n",
    "              label='eyewear')\n",
    "# use best_resnet50_model_5_layer_eyewear_0.25dobeta_2.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_testing(df=df,model_path='../tuning_data/resnet_data/resnet50_model_5_up.json',\n",
    "             weight_path='../tuning_data/resnet_data/untracked_resnet50/best_resnet50_model_5_layer_eyewear_0.25dobeta_2.h5',\n",
    "              label='eyewear')\n",
    "# best model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_testing(df,'../tuning_data/resnet_data/resnet50_model_5_up.json', \n",
    "              '../tuning_data/resnet_data/untracked_resnet50/best_resnet50_model_eyewear.h5',\n",
    "              'eyewear')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing hat "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_testing(df=df,model_path='../tuning_data/resnet_data/resnet50_model_5_up.json',\n",
    "             weight_path='../tuning_data/resnet_data/untracked_resnet50/best_resnet50_model_5_layer_hat_0.25dobeta_2.h5',\n",
    "              label='hat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_testing(df,'../tuning_data/resnet_data/resnet50_model_5_up.json', \n",
    "              '../tuning_data/resnet_data/untracked_resnet50/best_resnet50_model_hat.h5',\n",
    "             'hat')\n",
    "\n",
    "# best resnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing beard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_testing(df=df,model_path='../tuning_data/resnet_data/resnet50_model_5_up.json',\n",
    "             weight_path='../tuning_data/resnet_data/untracked_resnet50/best_resnet50_model_5_layer_beard_0.25dobeta_2.h5',\n",
    "              label='beard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_testing(df,'../tuning_data/resnet_data/resnet50_model_5_up.json',\n",
    "              '../tuning_data/resnet_data/untracked_resnet50/best_resnet50_model_beard.h5','beard')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
